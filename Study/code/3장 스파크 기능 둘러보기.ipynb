{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format('csv')\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option('header', 'true')\\\n",
    "    .load(\"../data/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+\n",
      "|CustomerId|              window|   sum(total_cost)|\n",
      "+----------+--------------------+------------------+\n",
      "|   15291.0|{2010-12-01 09:00...|             328.8|\n",
      "|   16098.0|{2010-12-01 09:00...|430.59999999999997|\n",
      "|   18074.0|{2010-12-01 09:00...|             489.6|\n",
      "|   17924.0|{2010-12-01 10:00...|             279.0|\n",
      "|   15862.0|{2010-12-01 11:00...| 354.2299999999999|\n",
      "|      null|{2010-12-01 11:00...|               0.0|\n",
      "|   13758.0|{2010-12-01 12:00...|362.45000000000005|\n",
      "|   13694.0|{2010-12-01 12:00...|            842.12|\n",
      "|   17968.0|{2010-12-01 12:00...|277.34999999999997|\n",
      "|   17377.0|{2010-12-01 12:00...|223.90000000000003|\n",
      "+----------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "\n",
    "staticDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hour\"))\\\n",
    "    .sum(\"total_cost\")\\\n",
    "    .show(10)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"../data/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "  .selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(UnitPrice * Quantity) as total_cost\",\n",
    "    \"InvoiceDate\")\\\n",
    "  .groupBy(\n",
    "    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 hours\"))\\\n",
    "  .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x265c2e2c490>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+\n",
      "|CustomerId|window|sum(total_cost)|\n",
      "+----------+------+---------------+\n",
      "+----------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM customer_purchases\n",
    "  ORDER BY `sum(total_cost)` DESC\n",
    "  \"\"\")\\\n",
    "  .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "# 원래는 'hh'가 아니라 'EEEE' 였으나 2010-12-01 데이터만 가져왔으므로 시간별로 나눠야 함\n",
    "preppedDataFrame = staticDataFrame\\\n",
    "  .na.fill(0)\\\n",
    "  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"hh\"))\\\n",
    "  .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽는 양이 많아 2010-12-01.csv만 읽었으므로 시간을 다시 조절해야함\n",
    "trainDataFrame = preppedDataFrame\\\n",
    "  .where(\"InvoiceDate < '2010-12-01 12:00:00'\")\n",
    "testDataFrame = preppedDataFrame\\\n",
    "  .where(\"InvoiceDate >= '2010-12-01 12:00:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|day_of_week|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   536420|    21889|WOODEN BOX OF DOM...|      12|2010-12-01 12:03:00|     1.25|   16583.0|United Kingdom|         12|\n",
      "|   536420|    21892|TRADITIONAL WOODE...|      12|2010-12-01 12:03:00|     1.25|   16583.0|United Kingdom|         12|\n",
      "|   536420|    21891|TRADITIONAL WOODE...|      12|2010-12-01 12:03:00|     1.25|   16583.0|United Kingdom|         12|\n",
      "|   536420|    21890|S/6 WOODEN SKITTL...|       6|2010-12-01 12:03:00|     2.95|   16583.0|United Kingdom|         12|\n",
      "|   536420|    21718|RED METAL BEACH S...|      12|2010-12-01 12:03:00|     1.25|   16583.0|United Kingdom|         12|\n",
      "|   536420|    21716|BOYS VINTAGE TIN ...|       8|2010-12-01 12:03:00|     2.55|   16583.0|United Kingdom|         12|\n",
      "|   536420|    21715|GIRLS VINTAGE TIN...|       8|2010-12-01 12:03:00|     2.55|   16583.0|United Kingdom|         12|\n",
      "|   536420|    22113|GREY HEART HOT WA...|       4|2010-12-01 12:03:00|     3.75|   16583.0|United Kingdom|         12|\n",
      "|   536420|    22111|SCOTTIE DOG HOT W...|       3|2010-12-01 12:03:00|     4.95|   16583.0|United Kingdom|         12|\n",
      "|   536420|    22110|BIRD HOUSE HOT WA...|       6|2010-12-01 12:03:00|     2.55|   16583.0|United Kingdom|         12|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDataFrame.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서 요일(day_of_week)는 범주형이므로 onehotencdoer로 실행해야함 -> 밑의 블록\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer()\\\n",
    "  .setInputCol(\"day_of_week\")\\\n",
    "  .setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\\\n",
    "  .setInputCol(\"day_of_week_index\")\\\n",
    "  .setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "  .setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans()\\\n",
    "    .setK(20)\\\n",
    "    .setSeed(333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTest = fittedPipeline.transform(testDataFrame)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f40cd846c2b1788046c62232ff40f4e411a1bca47201415c13fbe6b71cd8494"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
